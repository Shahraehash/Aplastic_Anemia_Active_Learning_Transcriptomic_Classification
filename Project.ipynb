{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U1fbUjw2ab6j"
   },
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 1821,
     "status": "ok",
     "timestamp": 1742310746490,
     "user": {
      "displayName": "Raehash Shah",
      "userId": "05264693677458722722"
     },
     "user_tz": 240
    },
    "id": "6CTcpwcaZ7x4"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 52187,
     "status": "ok",
     "timestamp": 1742310798675,
     "user": {
      "displayName": "Raehash Shah",
      "userId": "05264693677458722722"
     },
     "user_tz": 240
    },
    "id": "fScQOpFwWt3W"
   },
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv('GSE145668_combined_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 668,
     "status": "ok",
     "timestamp": 1742310799340,
     "user": {
      "displayName": "Raehash Shah",
      "userId": "05264693677458722722"
     },
     "user_tz": 240
    },
    "id": "EXnYWDwnXXzC"
   },
   "outputs": [],
   "source": [
    "data = raw_data.drop(columns=['GeneName','GeneSymbol'])\n",
    "data = data.transpose()\n",
    "data.columns = raw_data['GeneSymbol']\n",
    "data['Label'] = np.where(data.index.str.contains('Ctrl', na=False), 'Ctrl', 'AA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>GeneSymbol</th>\n",
       "      <th>A1BG</th>\n",
       "      <th>A1CF</th>\n",
       "      <th>A2M</th>\n",
       "      <th>A2ML1</th>\n",
       "      <th>A4GALT</th>\n",
       "      <th>AAAS</th>\n",
       "      <th>AACS</th>\n",
       "      <th>AADAC</th>\n",
       "      <th>AADAT</th>\n",
       "      <th>AAED1</th>\n",
       "      <th>...</th>\n",
       "      <th>ZWINT</th>\n",
       "      <th>ZXDA</th>\n",
       "      <th>ZXDB</th>\n",
       "      <th>ZXDC</th>\n",
       "      <th>ZYG11A</th>\n",
       "      <th>ZYG11B</th>\n",
       "      <th>ZYX</th>\n",
       "      <th>ZZEF1</th>\n",
       "      <th>ZZZ3</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Ctrl1_L1_sc01</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Ctrl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ctrl1_L1_sc02</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Ctrl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ctrl1_L1_sc03</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Ctrl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ctrl1_L1_sc04</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Ctrl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ctrl1_L1_sc05</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Ctrl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P9_L3_sc92</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>AA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P9_L3_sc93</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>AA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P9_L3_sc94</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>AA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P9_L3_sc95</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>AA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P9_L3_sc96</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>AA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8962 rows Ã— 17283 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "GeneSymbol     A1BG  A1CF  A2M  A2ML1  A4GALT  AAAS  AACS  AADAC  AADAT  \\\n",
       "Ctrl1_L1_sc01     0     0    0      0       0     0     0      0      0   \n",
       "Ctrl1_L1_sc02     0     0    0      0       0     0     0      0      0   \n",
       "Ctrl1_L1_sc03     0     0    0      0       0     0     0      0      0   \n",
       "Ctrl1_L1_sc04     0     0    0      0       0     0     0      0      0   \n",
       "Ctrl1_L1_sc05     0     0    0      0       0     0     0      0      0   \n",
       "...             ...   ...  ...    ...     ...   ...   ...    ...    ...   \n",
       "P9_L3_sc92        0     0    0      0       0     0     0      0      0   \n",
       "P9_L3_sc93        0     0    0      0       0     0     0      0      0   \n",
       "P9_L3_sc94        0     0    0      0       0     0     0      0      0   \n",
       "P9_L3_sc95        0     0    0      0       0     0     0      0      0   \n",
       "P9_L3_sc96        0     0    0      0       0     0     0      0      0   \n",
       "\n",
       "GeneSymbol     AAED1  ...  ZWINT  ZXDA  ZXDB  ZXDC  ZYG11A  ZYG11B  ZYX  \\\n",
       "Ctrl1_L1_sc01      0  ...      0     0     0     0       0       0    2   \n",
       "Ctrl1_L1_sc02      0  ...      0     0     0     0       0       0    0   \n",
       "Ctrl1_L1_sc03      0  ...      0     0     0     0       0       0    0   \n",
       "Ctrl1_L1_sc04      0  ...      0     0     0     0       0       1    1   \n",
       "Ctrl1_L1_sc05      0  ...      0     0     0     0       0       0    0   \n",
       "...              ...  ...    ...   ...   ...   ...     ...     ...  ...   \n",
       "P9_L3_sc92         0  ...      0     0     0     0       0       0    0   \n",
       "P9_L3_sc93         0  ...      0     0     0     0       0       0    0   \n",
       "P9_L3_sc94         0  ...      0     0     0     0       0       0    1   \n",
       "P9_L3_sc95         0  ...      0     0     0     0       0       0    0   \n",
       "P9_L3_sc96         0  ...      0     0     0     0       0       0    0   \n",
       "\n",
       "GeneSymbol     ZZEF1  ZZZ3  Label  \n",
       "Ctrl1_L1_sc01      0     0   Ctrl  \n",
       "Ctrl1_L1_sc02      0     0   Ctrl  \n",
       "Ctrl1_L1_sc03      0     0   Ctrl  \n",
       "Ctrl1_L1_sc04      0     0   Ctrl  \n",
       "Ctrl1_L1_sc05      0     0   Ctrl  \n",
       "...              ...   ...    ...  \n",
       "P9_L3_sc92         0     0     AA  \n",
       "P9_L3_sc93         0     0     AA  \n",
       "P9_L3_sc94         0     0     AA  \n",
       "P9_L3_sc95         0     0     AA  \n",
       "P9_L3_sc96         0     0     AA  \n",
       "\n",
       "[8962 rows x 17283 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Selection Method Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_batch_fisher_information(X_candidates, model, uncertainty_scores):\n",
    "    fisher_scores = np.zeros(len(X_candidates))\n",
    "    \n",
    "    # Calculate feature-wise scores without constructing the full matrix\n",
    "    for i in range(len(X_candidates)):\n",
    "        x = X_candidates[i]\n",
    "        proba = model.predict_proba(x.reshape(1, -1))[0, 1]\n",
    "        \n",
    "        # Clip probabilities to avoid numerical issues\n",
    "        p = np.clip(proba, 1e-6, 1-1e-6)\n",
    "        \n",
    "        # For logistic regression, diagonal of Fisher Information is p(1-p)*x^2\n",
    "        fisher_diag = p * (1 - p) * np.square(x)\n",
    "        \n",
    "        # Add a small offset to prevent zeros\n",
    "        fisher_score = np.sum(fisher_diag) + 1e-10\n",
    "        \n",
    "        # Combine with uncertainty score\n",
    "        fisher_scores[i] = uncertainty_scores[i] * fisher_score\n",
    "    \n",
    "    # Normalize scores to [0,1] range\n",
    "    if np.max(fisher_scores) > 0:\n",
    "        fisher_scores = fisher_scores / np.max(fisher_scores)\n",
    "    else:\n",
    "        # If all scores are zero, use uniform distribution\n",
    "        fisher_scores = np.ones(len(fisher_scores)) / len(fisher_scores)\n",
    "    \n",
    "    return fisher_scores\n",
    "\n",
    "def select_batch_min_fisher(indices, fisher_scores, batch_size):\n",
    "    sorted_indices = np.argsort(-fisher_scores)  \n",
    "    selected_batch = sorted_indices[:batch_size]\n",
    "    return indices[selected_batch]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XrRdVVOMdw6a"
   },
   "source": [
    "# Baseline Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jnSxy7KfaavS"
   },
   "source": [
    "## Passive Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from tqdm import tqdm\n",
    "\n",
    "def random_sampling(data, n_simulations, n_folds, n_init, n_term, batch_size, seed):\n",
    "    X = data.drop(columns=['Label']).values\n",
    "    y = data['Label'].values\n",
    "\n",
    "    training_results = np.zeros((n_folds, n_simulations, (n_term - n_init) // batch_size + 1))\n",
    "    testing_results = np.zeros((n_folds, n_simulations, (n_term - n_init) // batch_size + 1))\n",
    "\n",
    "    kf = KFold(n_splits=n_folds, shuffle=True, random_state=seed)\n",
    "    for fold_idx, (train_idx, test_idx) in enumerate(kf.split(X)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "        for iteration in tqdm(range(n_simulations)):\n",
    "            indices = np.arange(len(X_train))\n",
    "            starting_indices = np.random.choice(indices, size=n_init, replace=False)\n",
    "\n",
    "            X_start_train = X_train[starting_indices]\n",
    "            y_start_train = y_train[starting_indices]\n",
    "\n",
    "            indices = np.setdiff1d(indices, starting_indices)\n",
    "            \n",
    "            batch_counter = 0\n",
    "            model = SGDClassifier(loss=\"log_loss\", random_state=seed)\n",
    "            model.fit(X_start_train, y_start_train)\n",
    "            training_results[fold_idx, iteration, batch_counter] = model.score(X_start_train, y_start_train)\n",
    "            testing_results[fold_idx, iteration, batch_counter] = model.score(X_test, y_test)\n",
    "            batch_counter += 1\n",
    "\n",
    "            while len(X_start_train) < n_term and len(indices) >= batch_size:\n",
    "                # Select a random batch\n",
    "                batch_indices = np.random.choice(indices, size=batch_size, replace=False)\n",
    "                indices = np.setdiff1d(indices, batch_indices)\n",
    "                \n",
    "                X_batch = X_train[batch_indices]\n",
    "                y_batch = y_train[batch_indices]\n",
    "                \n",
    "                X_start_train = np.vstack([X_start_train, X_batch])\n",
    "                y_start_train = np.append(y_start_train, y_batch)\n",
    "                \n",
    "                model = SGDClassifier(loss=\"log_loss\", random_state=seed)\n",
    "                model.fit(X_start_train, y_start_train)\n",
    "                training_results[fold_idx, iteration, batch_counter] = model.score(X_start_train, y_start_train)\n",
    "                testing_results[fold_idx, iteration, batch_counter] = model.score(X_test, y_test)\n",
    "                batch_counter += 1\n",
    "\n",
    "    return training_results, testing_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LNYrYXefbljF"
   },
   "source": [
    "## Uncertainty Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from tqdm import tqdm\n",
    "\n",
    "def uncertainty_sampling(data, n_simulations, n_folds, n_init, n_term, batch_size, seed):\n",
    "    X = data.drop(columns=['Label']).values\n",
    "    y = data['Label'].values\n",
    "\n",
    "    feature_names = data.drop(columns=['Label']).columns\n",
    "\n",
    "    training_results = np.zeros((n_folds, n_simulations, (n_term - n_init) // batch_size + 1))\n",
    "    testing_results = np.zeros((n_folds, n_simulations, (n_term - n_init) // batch_size + 1))\n",
    "\n",
    "    feature_selection_counts = {feature: 0 for feature in feature_names}\n",
    "    feature_importance_values = {feature: [] for feature in feature_names}\n",
    "\n",
    "    kf = KFold(n_splits=n_folds, shuffle=True, random_state=seed)\n",
    "    for fold_idx, (train_idx, test_idx) in enumerate(kf.split(X)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "        for iteration in tqdm(range(n_simulations)):\n",
    "            indices = np.arange(len(X_train))\n",
    "            starting_indices = np.random.choice(indices, size=n_init, replace=False)\n",
    "\n",
    "            X_start_train = X_train[starting_indices]\n",
    "            y_start_train = y_train[starting_indices]\n",
    "\n",
    "            indices = np.setdiff1d(indices, starting_indices)\n",
    "            \n",
    "            batch_counter = 0\n",
    "            model = SGDClassifier(loss=\"log_loss\", random_state=seed)\n",
    "            model.fit(X_start_train, y_start_train)\n",
    "            training_results[fold_idx, iteration, batch_counter] = model.score(X_start_train, y_start_train)\n",
    "            testing_results[fold_idx, iteration, batch_counter] = model.score(X_test, y_test)\n",
    "            batch_counter += 1\n",
    "\n",
    "            while len(X_start_train) < n_term and len(indices) >= batch_size:\n",
    "                model = SGDClassifier(loss=\"log_loss\", random_state=seed)\n",
    "                model.fit(X_start_train, y_start_train)\n",
    "                \n",
    "                # Get uncertainty scores for remaining samples\n",
    "                probabilities = model.predict_proba(X_train[indices])[:, 1]\n",
    "                uncertainty_scores = np.abs(probabilities - 0.5)  # Distance from decision boundary\n",
    "                \n",
    "                # Compute Fisher Information Matrix for each candidate sample\n",
    "                fisher_scores = calculate_batch_fisher_information(X_train[indices], model, uncertainty_scores)\n",
    "                \n",
    "                # Select batch that minimizes Fisher Information\n",
    "                batch_indices = select_batch_min_fisher(indices, fisher_scores, batch_size)\n",
    "                indices = np.setdiff1d(indices, batch_indices)\n",
    "                \n",
    "                X_batch = X_train[batch_indices]\n",
    "                y_batch = y_train[batch_indices]\n",
    "\n",
    "                selected_samples = X_train[batch_indices]\n",
    "            \n",
    "                for i, feature in enumerate(feature_names):\n",
    "                    feature_importance_values[feature].extend(np.abs(selected_samples[:, i]).tolist())\n",
    "                    significant_values = np.abs(selected_samples[:, i]) > np.mean(np.abs(X_train[:, i]))\n",
    "                    feature_selection_counts[feature] += np.sum(significant_values)\n",
    "\n",
    "                \n",
    "                X_start_train = np.vstack([X_start_train, X_batch])\n",
    "                y_start_train = np.append(y_start_train, y_batch)\n",
    "                \n",
    "                model = SGDClassifier(loss=\"log_loss\", random_state=seed)\n",
    "                model.fit(X_start_train, y_start_train)\n",
    "                training_results[fold_idx, iteration, batch_counter] = model.score(X_start_train, y_start_train)\n",
    "                testing_results[fold_idx, iteration, batch_counter] = model.score(X_test, y_test)\n",
    "                batch_counter += 1\n",
    "    \n",
    "    avg_importance = {}\n",
    "    for feature in feature_names:\n",
    "        if feature_importance_values[feature]:\n",
    "            avg_importance[feature] = np.mean(feature_importance_values[feature])\n",
    "        else:\n",
    "            avg_importance[feature] = 0\n",
    "            \n",
    "    print(\"\\nTop 10 critical features for Uncertainty Sampling:\")\n",
    "    sorted_features = sorted(avg_importance.items(), key=lambda x: x[1], reverse=True)\n",
    "    for feature, importance in sorted_features[:10]:\n",
    "        print(f\"  {feature}: {importance:.4f} (selected {feature_selection_counts[feature]} times)\")\n",
    "    \n",
    "\n",
    "    return training_results, testing_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x-1-qYq-cGqG"
   },
   "source": [
    "## Query By Committee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from scipy.stats import entropy\n",
    "from tqdm import tqdm\n",
    "\n",
    "def qbc_sampling(data, n_simulations, n_folds, n_init, n_term, n_committee, batch_size, seed):\n",
    "    X = data.drop(columns=['Label']).values\n",
    "    y = data['Label'].values\n",
    "    feature_names = data.drop(columns=['Label']).columns\n",
    "\n",
    "    training_results = np.zeros((n_folds, n_simulations, (n_term - n_init) // batch_size + 1))\n",
    "    testing_results = np.zeros((n_folds, n_simulations, (n_term - n_init) // batch_size + 1))\n",
    "\n",
    "    feature_selection_counts = {feature: 0 for feature in feature_names}\n",
    "    feature_importance_values = {feature: [] for feature in feature_names}\n",
    "\n",
    "    kf = KFold(n_splits=n_folds, shuffle=True, random_state=seed)\n",
    "    for fold_idx, (train_idx, test_idx) in enumerate(kf.split(X)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "        for iteration in tqdm(range(n_simulations)):\n",
    "            indices = np.arange(len(X_train))\n",
    "            starting_indices = np.random.choice(indices, size=n_init, replace=False)\n",
    "\n",
    "            X_start_train = X_train[starting_indices]\n",
    "            y_start_train = y_train[starting_indices]\n",
    "\n",
    "            indices = np.setdiff1d(indices, starting_indices)\n",
    "            \n",
    "            batch_counter = 0\n",
    "            model = SGDClassifier(loss=\"log_loss\", random_state=seed)\n",
    "            model.fit(X_start_train, y_start_train)\n",
    "            training_results[fold_idx, iteration, batch_counter] = model.score(X_start_train, y_start_train)\n",
    "            testing_results[fold_idx, iteration, batch_counter] = model.score(X_test, y_test)\n",
    "            batch_counter += 1\n",
    "\n",
    "            while len(X_start_train) < n_term and len(indices) >= batch_size:\n",
    "                \n",
    "                committee = [SGDClassifier(loss=\"log_loss\", random_state=seed + j) for j in range(n_committee)]\n",
    "                for model in committee:\n",
    "                    model.fit(X_start_train, y_start_train)\n",
    "                \n",
    "                # Calculate disagreement for each sample\n",
    "                committee_probs = np.array([model.predict_proba(X_train[indices]) for model in committee])\n",
    "                committee_probs = np.transpose(committee_probs, (1, 0, 2))  # (samples, models, classes)\n",
    "                mean_pred_probs = np.mean(committee_probs, axis=1)\n",
    "\n",
    "                # Calculate KL divergence for each sample and model\n",
    "                disagreement_scores = []\n",
    "                for i in range(committee_probs.shape[0]):\n",
    "                    sample_kl = []\n",
    "                    for j in range(committee_probs.shape[1]):\n",
    "                        p = np.clip(committee_probs[i, j], 1e-10, 1)\n",
    "                        q = np.clip(mean_pred_probs[i], 1e-10, 1)\n",
    "                        sample_kl.append(entropy(p, q))\n",
    "                    disagreement_scores.append(np.mean(sample_kl))\n",
    "                disagreement_scores = np.array(disagreement_scores)\n",
    "                \n",
    "                # Compute Fisher Information Matrix for each candidate sample based on disagreement\n",
    "                fisher_scores = calculate_batch_fisher_information(X_train[indices], committee[0], disagreement_scores)\n",
    "                \n",
    "                # Select batch that minimizes Fisher Information\n",
    "                batch_indices = select_batch_min_fisher(indices, fisher_scores, batch_size)\n",
    "                indices = np.setdiff1d(indices, batch_indices)\n",
    "                \n",
    "                X_batch = X_train[batch_indices]\n",
    "                y_batch = y_train[batch_indices]\n",
    "\n",
    "                selected_samples = X_train[batch_indices]\n",
    "            \n",
    "                for i, feature in enumerate(feature_names):\n",
    "                    feature_importance_values[feature].extend(np.abs(selected_samples[:, i]).tolist())\n",
    "                    significant_values = np.abs(selected_samples[:, i]) > np.mean(np.abs(X_train[:, i]))\n",
    "                    feature_selection_counts[feature] += np.sum(significant_values)\n",
    "                \n",
    "                X_start_train = np.vstack([X_start_train, X_batch])\n",
    "                y_start_train = np.append(y_start_train, y_batch)\n",
    "                \n",
    "                model = SGDClassifier(loss=\"log_loss\", random_state=seed)\n",
    "                model.fit(X_start_train, y_start_train)\n",
    "                training_results[fold_idx, iteration, batch_counter] = model.score(X_start_train, y_start_train)\n",
    "                testing_results[fold_idx, iteration, batch_counter] = model.score(X_test, y_test)\n",
    "                batch_counter += 1\n",
    "    \n",
    "    avg_importance = {}\n",
    "    for feature in feature_names:\n",
    "        if feature_importance_values[feature]:\n",
    "            avg_importance[feature] = np.mean(feature_importance_values[feature])\n",
    "        else:\n",
    "            avg_importance[feature] = 0\n",
    "    \n",
    "    print(\"\\nTop 10 critical features for Query-by-Committee:\")\n",
    "    sorted_features = sorted(avg_importance.items(), key=lambda x: x[1], reverse=True)\n",
    "    for feature, importance in sorted_features[:10]:\n",
    "        print(f\"  {feature}: {importance:.4f} (selected {feature_selection_counts[feature]} times)\")\n",
    "    \n",
    "    return training_results, testing_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fuzzy KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from scipy.spatial.distance import cdist\n",
    "from tqdm import tqdm\n",
    "\n",
    "def fuzzy_knn_sampling(data, n_simulations, n_folds, n_init, n_term, batch_size, seed, k=5, m=2):\n",
    "    X = data.drop(columns=['Label']).values\n",
    "    y = data['Label'].values\n",
    "    feature_names = data.drop(columns=['Label']).columns\n",
    "    \n",
    "    unique_labels = np.unique(y)\n",
    "    if not np.array_equal(np.sort(unique_labels), np.array([0, 1])):\n",
    "        label_map = {unique_labels[0]: 0, unique_labels[1]: 1}\n",
    "        y = np.array([label_map[label] for label in y])\n",
    "\n",
    "    training_results = np.zeros((n_folds, n_simulations, (n_term - n_init) // batch_size + 1))\n",
    "    testing_results = np.zeros((n_folds, n_simulations, (n_term - n_init) // batch_size + 1))\n",
    "    \n",
    "    # Track selected features and their importance\n",
    "    feature_selection_counts = {feature: 0 for feature in feature_names}\n",
    "    feature_importance_values = {feature: [] for feature in feature_names}\n",
    "    \n",
    "    kf = KFold(n_splits=n_folds, shuffle=True, random_state=seed)\n",
    "    for fold_idx, (train_idx, test_idx) in enumerate(kf.split(X)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "        \n",
    "        for iteration in tqdm(range(n_simulations)):\n",
    "            indices = np.arange(len(X_train))\n",
    "            starting_indices = np.random.choice(indices, size=n_init, replace=False)\n",
    "            \n",
    "            X_start_train = X_train[starting_indices]\n",
    "            y_start_train = y_train[starting_indices]\n",
    "            \n",
    "            indices = np.setdiff1d(indices, starting_indices)\n",
    "            \n",
    "            batch_counter = 0\n",
    "            model = SGDClassifier(loss=\"log_loss\", random_state=seed)\n",
    "            model.fit(X_start_train, y_start_train)\n",
    "            \n",
    "            training_results[fold_idx, iteration, batch_counter] = model.score(X_start_train, y_start_train)\n",
    "            testing_results[fold_idx, iteration, batch_counter] = model.score(X_test, y_test)\n",
    "            batch_counter += 1\n",
    "            \n",
    "            while len(X_start_train) < n_term and len(indices) >= batch_size:\n",
    "                # Get unlabeled samples\n",
    "                X_unlabeled = X_train[indices]\n",
    "                \n",
    "                # For binary classification, we can simplify the fuzzy KNN approach\n",
    "                # by focusing on the membership to class 1\n",
    "                \n",
    "                # Calculate distances between unlabeled and labeled samples\n",
    "                distances = cdist(X_unlabeled, X_start_train, 'euclidean')\n",
    "                \n",
    "                # For each unlabeled sample, get the k nearest neighbors\n",
    "                nearest_neighbors = np.argsort(distances, axis=1)[:, :k]\n",
    "                nearest_distances = np.take_along_axis(distances, nearest_neighbors, axis=1)\n",
    "                \n",
    "                # Apply fuzzy distance weighting\n",
    "                # Avoid division by zero by adding a small epsilon\n",
    "                weights = 1.0 / (nearest_distances ** (2 / (m - 1)) + 1e-10)\n",
    "                \n",
    "                # Normalize weights\n",
    "                row_sums = weights.sum(axis=1, keepdims=True)\n",
    "                weights = weights / row_sums\n",
    "                \n",
    "                # Calculate fuzzy membership to class 1\n",
    "                membership_class_1 = np.zeros(len(X_unlabeled))\n",
    "                \n",
    "                for i in range(len(X_unlabeled)):\n",
    "                    neighbor_labels = y_start_train[nearest_neighbors[i]]\n",
    "                    membership_class_1[i] = np.sum(weights[i] * neighbor_labels)\n",
    "                \n",
    "                # For binary classification, uncertainty is highest at 0.5\n",
    "                uncertainty_scores = 0.5 - np.abs(membership_class_1 - 0.5)\n",
    "                \n",
    "                # Select the most uncertain samples\n",
    "                batch_indices = np.argsort(uncertainty_scores)[-batch_size:]\n",
    "                \n",
    "    \n",
    "                original_batch_indices = indices[batch_indices]\n",
    "                indices = np.setdiff1d(indices, original_batch_indices)\n",
    "            \n",
    "                selected_samples = X_train[original_batch_indices]\n",
    "                for i, feature in enumerate(feature_names):\n",
    "                    feature_importance_values[feature].extend(np.abs(selected_samples[:, i]).tolist())\n",
    "                    significant_values = np.abs(selected_samples[:, i]) > np.mean(np.abs(X_train[:, i]))\n",
    "                    feature_selection_counts[feature] += np.sum(significant_values)\n",
    "                \n",
    "                X_batch = X_train[original_batch_indices]\n",
    "                y_batch = y_train[original_batch_indices]\n",
    "                \n",
    "                X_start_train = np.vstack([X_start_train, X_batch])\n",
    "                y_start_train = np.append(y_start_train, y_batch)\n",
    "                \n",
    "\n",
    "                model = SGDClassifier(loss=\"log_loss\", random_state=seed)\n",
    "                model.fit(X_start_train, y_start_train)\n",
    "                \n",
    "                training_results[fold_idx, iteration, batch_counter] = model.score(X_start_train, y_start_train)\n",
    "                testing_results[fold_idx, iteration, batch_counter] = model.score(X_test, y_test)\n",
    "                batch_counter += 1\n",
    "    \n",
    "\n",
    "    avg_importance = {}\n",
    "    for feature in feature_names:\n",
    "        if feature_importance_values[feature]:\n",
    "            avg_importance[feature] = np.mean(feature_importance_values[feature])\n",
    "        else:\n",
    "            avg_importance[feature] = 0\n",
    "\n",
    "    print(\"\\nTop 10 critical features for Fuzzy KNN Sampling:\")\n",
    "    sorted_features = sorted(avg_importance.items(), key=lambda x: x[1], reverse=True)\n",
    "    for feature, importance in sorted_features[:10]:\n",
    "        print(f\"  {feature}: {importance:.4f} (selected {feature_selection_counts[feature]} times)\")\n",
    "    \n",
    "    return training_results, testing_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QUIRE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics.pairwise import rbf_kernel\n",
    "from numpy.linalg import pinv\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "\n",
    "def compute_quire_scores(X_all, labeled_indices, unlabeled_indices, gamma=1.0):\n",
    "    # Compute similarity matrix using RBF kernel\n",
    "    W = rbf_kernel(X_all, gamma=gamma)\n",
    "\n",
    "    # Construct Laplacian matrix\n",
    "    D = np.diag(W.sum(axis=1))\n",
    "    L = D - W\n",
    "\n",
    "    # Compute L^{-1} (pseudo-inverse for numerical stability)\n",
    "    L_inv = pinv(L + 1e-6 * np.eye(L.shape[0]))\n",
    "\n",
    "    # Compute QUIRE scores using the diagonal of the inverse Laplacian\n",
    "    scores = []\n",
    "    for idx in unlabeled_indices:\n",
    "        score = L_inv[idx, idx]\n",
    "        scores.append(score)\n",
    "\n",
    "    return np.array(scores)\n",
    "\n",
    "\n",
    "\n",
    "def quire_sampling(data, n_simulations, n_folds, n_init, n_term, batch_size, seed):\n",
    "    X = data.drop(columns=['Label']).values\n",
    "    y = data['Label'].values\n",
    "\n",
    "    training_results = np.zeros((n_folds, n_simulations, (n_term - n_init) // batch_size + 1))\n",
    "    testing_results = np.zeros((n_folds, n_simulations, (n_term - n_init) // batch_size + 1))\n",
    "\n",
    "    kf = KFold(n_splits=n_folds, shuffle=True, random_state=seed)\n",
    "    feature_counter = Counter()\n",
    "    for fold_idx, (train_idx, test_idx) in enumerate(kf.split(X)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "        for iteration in tqdm(range(n_simulations), desc=f\"Fold {fold_idx + 1}/{n_folds}\"):\n",
    "            indices = np.arange(len(X_train))\n",
    "            starting_indices = np.random.choice(indices, size=n_init, replace=False)\n",
    "\n",
    "            X_start_train = X_train[starting_indices]\n",
    "            y_start_train = y_train[starting_indices]\n",
    "\n",
    "            indices = np.setdiff1d(indices, starting_indices)\n",
    "\n",
    "            batch_counter = 0\n",
    "            model = SGDClassifier(loss=\"log_loss\", random_state=seed)\n",
    "            model.fit(X_start_train, y_start_train)\n",
    "            training_results[fold_idx, iteration, batch_counter] = model.score(X_start_train, y_start_train)\n",
    "            testing_results[fold_idx, iteration, batch_counter] = model.score(X_test, y_test)\n",
    "            batch_counter += 1\n",
    "\n",
    "            while len(X_start_train) < n_term and len(indices) >= batch_size:\n",
    "                model = SGDClassifier(loss=\"log_loss\", random_state=seed)\n",
    "                model.fit(X_start_train, y_start_train)\n",
    "\n",
    "                # Get current labeled and unlabeled indices\n",
    "                labeled_indices = np.setdiff1d(np.arange(len(X_train)), indices)\n",
    "                unlabeled_indices = indices\n",
    "\n",
    "                # Compute QUIRE scores\n",
    "                quire_scores = compute_quire_scores(X_train, labeled_indices, unlabeled_indices)\n",
    "\n",
    "                # Select samples with the lowest QUIRE scores\n",
    "                selected_relative_indices = np.argsort(quire_scores)[:batch_size]\n",
    "                batch_indices = unlabeled_indices[selected_relative_indices]\n",
    "\n",
    "                # Update indices\n",
    "                indices = np.setdiff1d(indices, batch_indices)\n",
    "\n",
    "                # Add selected batch to training set\n",
    "                X_batch = X_train[batch_indices]\n",
    "                y_batch = y_train[batch_indices]\n",
    "\n",
    "                for row in X_batch:\n",
    "                    top_feature_indices = np.argsort(np.abs(row))[-3:]  # top 3 important features\n",
    "                    for idx in top_feature_indices:\n",
    "                        feature_counter[idx] += 1\n",
    "\n",
    "                X_start_train = np.vstack([X_start_train, X_batch])\n",
    "                y_start_train = np.append(y_start_train, y_batch)\n",
    "\n",
    "                model = SGDClassifier(loss=\"log_loss\", random_state=seed)\n",
    "                model.fit(X_start_train, y_start_train)\n",
    "                training_results[fold_idx, iteration, batch_counter] = model.score(X_start_train, y_start_train)\n",
    "                testing_results[fold_idx, iteration, batch_counter] = model.score(X_test, y_test)\n",
    "                batch_counter += 1\n",
    "            \n",
    "            with open('quire_current.npy', 'wb') as f:\n",
    "                np.save(f, training_results)\n",
    "                np.save(f, testing_results)\n",
    "\n",
    "    top_features = feature_counter.most_common(10)\n",
    "    print(\"\\nTop 10 Most Important Features (based on frequency in selected samples):\")\n",
    "    for feature, count in top_features:\n",
    "        print(f\"Feature {feature} selected {count} times\")\n",
    "        \n",
    "    return training_results, testing_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import KFold\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "class DeepDropoutNN(nn.Module):\n",
    "    def __init__(self, input_dim, dropout_prob=0.5):\n",
    "        super(DeepDropoutNN, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_prob),\n",
    "\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_prob),\n",
    "\n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_prob),\n",
    "\n",
    "            nn.Linear(128, 2)  # Binary classification output logits\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "def predict_with_uncertainty(model, X, T=20):\n",
    "    model.train()  # Enable dropout during inference\n",
    "    softmax = nn.Softmax(dim=1)\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(T):\n",
    "            logits = model(X)\n",
    "            probs = softmax(logits)\n",
    "            predictions.append(probs.unsqueeze(0))\n",
    "\n",
    "    preds = torch.cat(predictions, dim=0)  # [T, batch, num_classes]\n",
    "    mean_probs = preds.mean(dim=0)\n",
    "    variance = preds.var(dim=0).mean(dim=1)\n",
    "    return mean_probs, variance\n",
    "\n",
    "\n",
    "def active_learning_dropout(data, n_simulations, n_folds, n_init, n_term, batch_size, seed):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    X = data.drop(columns=['Label']).values.astype(np.float32)\n",
    "    y = data['Label'].astype('category').cat.codes.values\n",
    "\n",
    "    training_results = np.zeros((n_folds, n_simulations, (n_term - n_init) // batch_size + 1))\n",
    "    testing_results = np.zeros((n_folds, n_simulations, (n_term - n_init) // batch_size + 1))\n",
    "\n",
    "    cell_counter = Counter()\n",
    "\n",
    "    kf = KFold(n_splits=n_folds, shuffle=True, random_state=seed)\n",
    "    for fold_idx, (train_idx, test_idx) in enumerate(kf.split(X)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "        X_test_tensor = torch.tensor(X_test)\n",
    "        y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "        for iteration in tqdm(range(n_simulations), desc=f\"Fold {fold_idx + 1}/{n_folds}\"):\n",
    "            indices = np.arange(len(X_train))\n",
    "            start_idx = np.random.choice(indices, size=n_init, replace=False)\n",
    "            indices = np.setdiff1d(indices, start_idx)\n",
    "\n",
    "            selected_X = torch.tensor(X_train[start_idx])\n",
    "            selected_y = torch.tensor(y_train[start_idx], dtype=torch.long)\n",
    "\n",
    "            batch_counter = 0\n",
    "            model = DeepDropoutNN(X.shape[1])\n",
    "            optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
    "            loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "            def train_model(model, X, y, epochs=25):\n",
    "                model.train()\n",
    "                for _ in range(epochs):\n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = model(X)\n",
    "                    loss = loss_fn(outputs, y)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "            train_model(model, selected_X, selected_y)\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                acc_train = (model(selected_X).argmax(dim=1) == selected_y).float().mean().item()\n",
    "                acc_test = (model(X_test_tensor).argmax(dim=1) == y_test_tensor).float().mean().item()\n",
    "            training_results[fold_idx, iteration, batch_counter] = acc_train\n",
    "            testing_results[fold_idx, iteration, batch_counter] = acc_test\n",
    "            batch_counter += 1\n",
    "\n",
    "            while len(selected_X) < n_term and len(indices) >= batch_size:\n",
    "                model.eval()\n",
    "                X_pool = torch.tensor(X_train[indices])\n",
    "                mean_probs, uncertainty = predict_with_uncertainty(model, X_pool, T=20)\n",
    "\n",
    "                topk = torch.topk(uncertainty, batch_size)\n",
    "                uncertain_indices = indices[topk.indices.numpy()]\n",
    "\n",
    "                for idx in uncertain_indices:\n",
    "                    global_idx = train_idx[idx]\n",
    "                    cell_counter[global_idx] += 1\n",
    "\n",
    "                batch_X = torch.tensor(X_train[uncertain_indices])\n",
    "                batch_y = torch.tensor(y_train[uncertain_indices], dtype=torch.long)\n",
    "\n",
    "                selected_X = torch.cat([selected_X, batch_X])\n",
    "                selected_y = torch.cat([selected_y, batch_y])\n",
    "                indices = np.setdiff1d(indices, uncertain_indices)\n",
    "\n",
    "                model = DeepDropoutNN(X.shape[1])\n",
    "                optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
    "                train_model(model, selected_X, selected_y, epochs=25)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    acc_train = (model(selected_X).argmax(dim=1) == selected_y).float().mean().item()\n",
    "                    acc_test = (model(X_test_tensor).argmax(dim=1) == y_test_tensor).float().mean().item()\n",
    "                training_results[fold_idx, iteration, batch_counter] = acc_train\n",
    "                testing_results[fold_idx, iteration, batch_counter] = acc_test\n",
    "                batch_counter += 1\n",
    "\n",
    "            with open('deep_mc_dropout_results.npy', 'wb') as f:\n",
    "                np.save(f, training_results)\n",
    "                np.save(f, testing_results)\n",
    "\n",
    "    print(\"\\nTop 10 Most Frequently Selected Cells:\")\n",
    "    for idx, count in cell_counter.most_common(10):\n",
    "        print(f\"Cell {idx} selected {count} times\")\n",
    "\n",
    "    return training_results, testing_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uMD3Q6h8d6FX"
   },
   "source": [
    "## Perform Sampling Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1742311246433,
     "user": {
      "displayName": "Raehash Shah",
      "userId": "05264693677458722722"
     },
     "user_tz": 240
    },
    "id": "AJ9OUSZad5YQ"
   },
   "outputs": [],
   "source": [
    "n_simulations = 3\n",
    "n_folds = 4\n",
    "n_init = int(len(data)/n_folds)\n",
    "n_term = int(len(data) * (n_folds - 1)/(n_folds))\n",
    "#n_init = 10\n",
    "#n_term = 15\n",
    "n_committee = 3\n",
    "seed = 28\n",
    "batch_size = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 49,
     "status": "ok",
     "timestamp": 1742311247246,
     "user": {
      "displayName": "Raehash Shah",
      "userId": "05264693677458722722"
     },
     "user_tz": 240
    },
    "id": "9eGvIlsdli8Q",
    "outputId": "81383fd0-0bb2-42b4-f0e4-847469ecf649"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2240, 6721, 150)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_init, n_term, batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 579978,
     "status": "ok",
     "timestamp": 1742311827705,
     "user": {
      "displayName": "Raehash Shah",
      "userId": "05264693677458722722"
     },
     "user_tz": 240
    },
    "id": "mTC9s0dte23V"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [07:20<00:00, 146.95s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [07:47<00:00, 155.76s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [07:40<00:00, 153.35s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [06:57<00:00, 139.03s/it]\n"
     ]
    }
   ],
   "source": [
    "passive_learning_training, passive_learning_testing = random_sampling(data, n_simulations, n_folds, n_init, n_term, batch_size, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nwith open('passive_learning_results.npy', 'wb') as f:\\n    np.save(f, passive_learning_training)\\n    np.save(f, passive_learning_testing)\\n\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.savez('passive_learning_results.npz', \n",
    "         training=passive_learning_training,\n",
    "         testing=passive_learning_testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [16:04<00:00, 321.64s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [16:51<00:00, 337.18s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [16:22<00:00, 327.63s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [16:29<00:00, 329.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 critical features for Uncertainty Sampling:\n",
      "  RPS27: 521.1275 (selected 14083 times)\n",
      "  RPLP1: 346.1210 (selected 14486 times)\n",
      "  MT-CO1: 340.7516 (selected 14040 times)\n",
      "  RPL13: 320.5124 (selected 14917 times)\n",
      "  RPS29: 312.1715 (selected 15543 times)\n",
      "  MT-CO3: 284.8146 (selected 13135 times)\n",
      "  TMSB4X: 284.6804 (selected 12227 times)\n",
      "  RPS14: 276.2634 (selected 14874 times)\n",
      "  RPL31: 260.7981 (selected 14565 times)\n",
      "  MT-CYB: 249.2302 (selected 13442 times)\n"
     ]
    }
   ],
   "source": [
    "uncertainty_sampling_training, uncertainty_sampling_testing = uncertainty_sampling(data, n_simulations, n_folds, n_init, n_term, batch_size, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nwith open('uncertainty_sampling.npy', 'wb') as f:\\n    np.save(f, uncertainty_sampling_training)\\n    np.save(f, uncertainty_sampling_testing)\\n\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.savez('uncertainty_sampling.npz', \n",
    "         training=uncertainty_sampling_training,\n",
    "         testing=uncertainty_sampling_testing) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [29:44<00:00, 594.81s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [31:08<00:00, 622.92s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [32:16<00:00, 645.40s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [26:26<00:00, 528.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 critical features for Query-by-Committee:\n",
      "  RPS27: 513.1295 (selected 13878 times)\n",
      "  RPLP1: 337.3948 (selected 13944 times)\n",
      "  MT-CO1: 332.1348 (selected 13592 times)\n",
      "  RPL13: 312.4957 (selected 14455 times)\n",
      "  RPS29: 305.5713 (selected 15238 times)\n",
      "  TMSB4X: 279.5582 (selected 12123 times)\n",
      "  MT-CO3: 278.4653 (selected 12797 times)\n",
      "  RPS14: 270.2800 (selected 14449 times)\n",
      "  RPL31: 254.9941 (selected 14193 times)\n",
      "  MT-CYB: 244.5677 (selected 13194 times)\n"
     ]
    }
   ],
   "source": [
    "qbc_sampling_training, qbc_sampling_testing = qbc_sampling(data, n_simulations, n_folds, n_init, n_term, n_committee, batch_size, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nwith open('query_by_committee.npy', 'wb') as f:\\n    np.save(f, qbc_sampling_training)\\n    np.save(f, qbc_sampling_testing)\\n\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.savez('query_by_committee.npz', \n",
    "         training=qbc_sampling_training,\n",
    "         testing=qbc_sampling_testing) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [1:06:41<00:00, 1333.74s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [1:06:15<00:00, 1325.18s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [1:11:29<00:00, 1430.00s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [1:31:45<00:00, 1835.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 critical features for Fuzzy KNN Sampling:\n",
      "  RPS27: 511.8756 (selected 13883 times)\n",
      "  RPLP1: 343.2138 (selected 14202 times)\n",
      "  MT-CO1: 335.6958 (selected 13766 times)\n",
      "  RPL13: 316.4106 (selected 14632 times)\n",
      "  RPS29: 308.0835 (selected 15252 times)\n",
      "  MT-CO3: 281.6321 (selected 12910 times)\n",
      "  TMSB4X: 277.7790 (selected 12078 times)\n",
      "  RPS14: 272.6684 (selected 14555 times)\n",
      "  RPL31: 258.0197 (selected 14298 times)\n",
      "  MT-CYB: 246.2987 (selected 13135 times)\n"
     ]
    }
   ],
   "source": [
    "fuzzy_knn_training, fuzzy_knn_testing = fuzzy_knn_sampling(data, n_simulations, n_folds, n_init, n_term, batch_size, seed, k=5, m=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez('fuzzy_knn_results.npz', \n",
    "         training=fuzzy_knn_training,\n",
    "         testing=fuzzy_knn_testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quire_training, quire_testing = quire_sampling(data, n_simulations, n_folds, n_init, n_term, batch_size, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez('quire_learning_results.npz', \n",
    "         training=quire_training,\n",
    "         testing=quire_testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_training, dl_testing = active_learning_dropout(data, n_simulations, n_folds, n_init, n_term, batch_size, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez('nn_learning_results.npz', \n",
    "         training=dl_training,\n",
    "         testing=dl_testing)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPOkW7IYOsFob10Nd4KS3nl",
   "collapsed_sections": [
    "U1fbUjw2ab6j",
    "jnSxy7KfaavS",
    "LNYrYXefbljF",
    "x-1-qYq-cGqG"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Automation_of_Scientific_Research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
